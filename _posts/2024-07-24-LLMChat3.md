---
title: Create an AI chatbot in python with me | Part 3, Our first script
date: 2024-07-24 12:44:00 +3:30
categories: [Coding, LLMchatbot]
tags: [ai, coding]     # TAG names should always be lowercase
description: We are going to write an AI chatbot in python.
---

# Approaches

There are many ways to build our app. Here are some suggestions:

- Use `llama.cpp` binaries and use python as an automation script to run OS commands

- Use `llama-server` binaries and use python as an HTTP client to interact with the server

- Use `llama-cpp-python` to call functions into our script
  
  

The last options is the wisest one. Here's why:

1. Using python to run OS commands would make our app really slow and also gives us fewer options to interact with the model

2. Running the server and use python to send HTTP reuquest to the server is also better but still it gives us less options compared to `llama-cpp-python`
   
   

So we take the last one. Our app would have two parts:

1. Backed

2. TUI

3. Runner
   
   

Backend is where we interact with the model... Think it as a server. And TUI is our frontend *(TUI stands for **T**erminal **U**ser **I**nterface)*. And runner would be one-click script to start it all. For now, lets start with our backed.



# Our first script: Backend

First, let's create a file called `backend.py` in our `src/` folder:

```shell
$PyChatbot/src touch backend.py
$PyChatbot/src subl backend.py
```

Now, let's import  `Llama` class from `llama-cpp-python`:

```python
from llama_cpp import Llama
```

Then, we want to load the model into RAM. We can use `Llama` class we imported:

```python
def load_model(model_path):

    model = Llama(model_path)
    return model
```

Now let's create a variable to hold the model path for us: *(better to put this before our load_model() function)*

```python
path_to_model = "../models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
```

Call the function

```python
if __name__ == "__main__":
    loaded_model = load_model(path_to_model)
```

Let's see if it works *(make sure venv is activated)*

```shell
(venv)$PyChatbot/src python backend.py
```

You will see a long output like this:

![2024-07-22-16-56-19-image](https://github.com/user-attachments/assets/6b8ab9b0-f1ed-4e54-bfeb-625166fc9bc9)


That means the script is working. The model loaded into our RAM and then shuts down.

`Llama` class has some parameters that helps us to optimize our model loading. Here are some important ones:

- n_ctx: This is the context window. It will basically determines how much the overall context of our chat would be. If it gets higher, it returns an error. Higher numbers will slow down the loading time and requires more system resources.

- mlock: Means **Memory Lock**. This will keep the model into RAM even if the app gets shutdown.

- chat_format: This is the format that model would respond to us.
  
  

So we need to save some configs into files, so the script could read them. Let's create a folder named `configs`

```shell
(venv)$PyChatbot/src mkdir ../configs
(venv)$PyChatbot/src cd ../configs
```

For now, let's save all these values into a file called `model_parameters.json` in `configs` folder

```json
{
    "n_ctx": 2048,
    "mlock": "True",
    "chat_format": "llama-3"
}
```

Now we'll create a function to read these parameters. We use `json` module to parse our json file

```python
import json
```

```python
def read_params():
    with open('../configs/model_parameters.json', 'r') as file:
        parameters = json.load(file)
    return parameters
```

Let's check if it's the correct format

```python
if __name__ == "__main__":
    params = read_params()
    print(params)
```

```shell
(venv)$PyChatbot/src python backend.py
```

![2024-07-22-19-42-44-image](https://github.com/user-attachments/assets/398f7565-4573-40dd-89dd-13bcbe7afe2f)


As we can see, it's in the correct format. so firstly let's change `"True"` to `True`

```python
def read_params():
	with open('../configs/model_parameters.json', 'r') as file:
		parameters = json.load(file)
	parameters['mlock'] = True
	return parameters
```

So now let's pass these to our model

```python
def load_model(model_path, params):
	model = Llama(model_path, mlock=params['mlock'], n_ctx=params['n_ctx'], chat_format=params['chat_format'])
	return model
```

```python
if __name__ == "__main__":
    params = read_params()
    loaded_model = load_model(path_to_model, params)  
```

Run it again and it works

```shell
(venv)$PyChatbot/src python backend.py
```

Now that we can load the model into our device, let's chat with it. we can use `create_chat_completion()` function from `Llama` class. 

Based on the documention, it will get a list of messages that could be the system prompt, user message and model message. This list is basically our chat memory. every message between user and model and system prompt will get saved into this list.

```python
def chat_comp(messages, model):
    model_output = model.create_chat_completion(messages=messages)
    return output
```

The format of `messages` list would be like this:

```python
messages = [
    {'role': 'system', 'content': instruction},
    {'role': 'user', 'content': our_message}
    {'role': 'assistant', 'content': model_response}
]
```

This way we can keep inserting our message and model response into the chat context.

Now let's test to see if the model responds to us

```python
if __name__ == "__main__":
    messages = [
    {'role': 'assistant', 'content': 'You are a helpful AI assistant'},
    {'role': 'user', 'content': 'Hey. How is it going?'}
    ]
    params = read_params()
    loaded_model = load_model(path_to_model, params)
    output = chat_comp(messages, loaded_model)
    print(output)
```

Now let's run it

```shell
(venv)$PyChatbot/src python backend.py
```

![2024-07-22-20-31-12-image](https://github.com/user-attachments/assets/76147287-bb69-41e8-a5da-73bbe4ee68c5)


As we can see, here's the model response

```json
{'id': 'chatcmpl-3da6d5a2-5ed4-47f4-9af6-68e14453d556', 'object': 'chat.completion', 'created': 1721663973, 'model': 'F:/PyChatbot/models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': "I'm doing well, thanks for asking! I'm here to help answer any questions you may have or provide assistance with any tasks you need help with. How about you? How's your day going so far?"}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 28, 'completion_tokens': 43, 'total_tokens': 71}}
```

Let's edit our code to only show the response text

```python
def chat_comp(messages, model):
	model_ouput = model.create_chat_completion(messages=messages)
	return model_ouput['choices'][0]['message']['content']
```

Let's run it again

![2024-07-22-20-38-03-image](https://github.com/user-attachments/assets/5a5b53a5-9586-4810-8220-7bf658bc4f61)


Yay! Only the model response is now printed. But we still have many improvements to do.

Now as the last step of this part, let's make it an actual chatbot that you can send message, get response and determine the system prompt yourself.

For now i make it so it gets the system prompt from sys args. For example:

```shell
(venv)$PyChatbot/src python backend.py You're an AI assistant
```

We can do it using `sys` module

```python
import sys
```

```python
def get_sys_promopt():
    return sys.argv[1]
```

That's it. but we still need to let use enter the message, wait for model to respond and add both of them to context. Here's a simplified view:

1. User enters the message

2. Adds the user message to context

3. send the context to model *(The whole context should be send to the model... It's bascially the chat memory)*

4. Model responds

5. Add The response to context

We can create a function to add everything to chat context

```python
def add_to_context(messages_list, message):
    messages_list.append(message)
```

It's time to create our runner file

```shell
(venv)$PyChatbot/src touch run.py
(venv)$PyChatbot/src subl run.py
```

Let's call our backend into our run.py file

```python
import backend
```

We can now call our backend functions in our runner file. For now, we want to load the model as soon as the script starts. so let's get model parameters from our file first

```python
model_params = backend.read_params()
```

Now let's save the model path into a variable *(we will edit it to be more responsive later. For now it should be ok for testing)*

```python
model_path = '../models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf'
loaded_model = backend.load_model(model_path, model_params)
```

Now let's create a function in our runner file to get user input and return it

```python
def get_input():
    return str(input("[+] USER: "))
```

Now let's add the user message to the context. so let's define a list as our context. This should be a global variable

```python
messages_list = []
```

**Now let's gather everything in our runner.py**

First we should read our system prompt from CLI arguments and add it to our context

```python
sys_prompt = backend.get_sys_prompt()
sys_context = {"role": "system", "content": sys_prompt}
backend.add_to_context(messages_list, sys_context)
```

Then we need to get user message continuously and add it to our context list.

```python
while True:
    user_message = get_input()
    user_context = {"role": "user", "content": user_message}
    backend.add_to_context(messages_list, user_message)
```

Now we can send our context and first message to mode, wait for model response, add the model response to our context and get user input again... This cycle will continue till user cancels it manually

```python
while True:
    user_message = get_input()
    user_context = {"role": "user", "content": user_message}
    backend.add_to_context(messages_list, user_message)
    response = backend.chat_comp(messages_list, loaded_model)
    print("[+] AI: " + response)
    response_context = {"role": "assistant", "content": response}
    backend.add_to_context(messages_list, response_context)
```

Let's run and test it

```shell
(venv)$PyChatbot/src python run.py "You're an AI assistant"
```

As you can see, the app runs normally but there are a lot of texts going on. 

![2024-07-24-01-35-09-image](https://github.com/user-attachments/assets/29339878-8048-4c87-996e-c07c9210c2e9)


These are the model loading's logs. We can disable this by adding this line to our `model_parameters.json`

```json
"verbose": "False"
```

So our `model_parameters.json` should look like this

```json
{
	"n_ctx": 2048,
	"mlock": "True",
	"chat_format": "llama-3",
	"verbose": "False"
}

```

Let's get back to our `backend.py` and change the `read_params()` function so the "True" and "False" becomes boolean

```python
def read_params():
	with open('../configs/model_parameters.json', 'r') as file:
		parameters = json.load(file)
    for key in parameters.keys():
        if parameters[key] == "True":
            parameters[key] = True
        if parameters[key] == "False":
            parameters[key] = False
	return parameters

```

Now that we disabled `verbose`, we can only see our messages and the model response.

We also need to change the `load_model()` function in our `backend.py` file to set to verbose to false. We'll optimize it later

```python
def load_model(model_path, params):
    model = Llama(model_path, mlock=params['mlock'], n_ctx=params['n_ctx'], chat_format=params['chat_format'], verbose=params['verbose'])
    return model
```

Let's test it again

```shell
(venv)$PyChatbot/src python run.py "You're an AI assistant"
```

![2024-07-24-01-41-23-image](https://github.com/user-attachments/assets/77cc8db1-6843-422c-9d19-4e9c15f3d632)


As we can see, it worked. I think it's enough for this part. We will do some optimizations to make it more user-friendly. We still don't have any TUI... That will be for last step i guess.
