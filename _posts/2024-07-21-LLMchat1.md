---
title: Create an AI chatbot in python with me | Part 1, Concept
date: 2024-07-21 13:05:00 +3:30
categories: [Coding, LLMchatbot]
tags: [ai, coding]     # TAG names should always be lowercase
description: We are going to write an AI chatbot in python.
---


# Introduction
ChatGPT, Copilot, Gemini... I'm pretty sure everyone used at least one of them. But there are limits to them. For example, all of these AIs are censored, and besides that, you need internet connection to access them. Also, to use all features, you need to buy subscription. 
Well, good news is you can have one of these AI chatbots in your own computer without needing internet connection or subscription... Well, the only requirement is good computational resources (good GPU or good CPU and enough RAM). Although we'll try our best to optimize it for older devices... But you still need decent power to run them.
# Why?
There are plenty of LLM runners out there including [Oobabooga's Text generation webui](https://github.com/oobabooga/text-generation-webui), [Backyard AI](https://backyard.ai/), [LM Studio](https://lmstudio.ai/) and so on. But here are some common problems with these runners:
 1. Starting time is really slow
 2. Sometimes it takes time to load the model into device
 3. Heavy (lots of python dependencies)
 4. Not best performance on old devices
 
 So my main reasons to build my own LLM loader are:
 
 5. Lightweight
 6. Faster on old devices
 7. Easy to use
 8. Less python libraries
 9. It's Fun!
 
 Well, maybe my reasons are not really valid, but hey, it would be fun.
 
# Concept
I assume that you know about LLMs and how they work, so I mainly focus on coding. But in short, LLMs (**L**arge **L**anguage **M**odels) are computational models that trained on big datasets (Mostly texts). They're some sort of AIs. Let's take an example... If you've chatted with ChatGPT, you know that it has several types of models: GPT3.5, GPT4 and GPT4o. These models are LLMs. In this case the difference between them is the datasets they've got trained on. The bigger the dataset is, the more intelligent the model becomes and more computational resources it needs. There are many open-source models out there such as Llama3, Gemma2, Mistral and so on. I may write a more comprehensive post about LLMs. But for now, this is enough knowledge.
 As you may ask, we are not going to build our own LLMs... They're hard to train and probably needs big datasets and good resources. What we are going to build would basically load a LLM into our device and gives us the ability to interact with it. It shouldn't be that hard... It's mostly python scripting.
 
 GUI? **NO**
 **Why?** Well, it's cooler to give it a hackery-terminal look... Also, I hate using mouse so i create it this way. 
 But hey, don't be frustrated. We will use `Curses` Python library to give a sexy TUI. (I hope so... i know nothing about Curses and never worked with it. But hey, we are here for learning new things), So normal users with less coding knowledge could use it too... 
 So, the final look should be something like this:
 
 ![image](https://github.com/user-attachments/assets/be883669-b94a-4a96-82ad-439d217b30d5)
 
*Picture is not mine*

 One side would be our chat window and the other would be for loading model, stdouts, errors and so on.
Here's a quick unprofessional diagram on how it works:
![diagram](https://github.com/user-attachments/assets/0d2e814a-f2d3-45c4-bccf-5a446dac27f2)


 1. Model loader. There are plenty of implementations we can use, such as `Llama-cpp`, `Transformers`, `Exllama` and so on. I've tested nearly all of them and Llama-cpp is faster on old devices and specially on CPUs. As we want to target old devices and they usually don't have modern GPUs, we will be using `Llama-cpp`
 2. Our app. This is what we will be coding. We use `Llama-cpp` to load models into our device and interact with the model. We'll write it in Python3.x as it is easier and more suitable than C/C++. 


 **I will link next parts here as soon as I publish them. Make sure to stay updated.**
