# Some tweakings

Well, our app is functioning normally right now... but c'mon this isn't what we wanted... We don't have any features yet. In this part we'll add some features to enhance the user experience... 

### Model Prameters

`Llama` class accepts many arguments. Here's from the documention:

```python
__init__(model_path, *, n_gpu_layers=0, 
split_mode=llama_cpp.LLAMA_SPLIT_MODE_LAYER, main_gpu=0, 
tensor_split=None, rpc_servers=None, 
vocab_only=False, use_mmap=True, 
use_mlock=False, kv_overrides=None, 
seed=llama_cpp.LLAMA_DEFAULT_SEED, n_ctx=512, n_batch=512, n_threads=None,
n_threads_batch=None, 
rope_scaling_type=llama_cpp.LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED, 
pooling_type=llama_cpp.LLAMA_POOLING_TYPE_UNSPECIFIED, 
rope_freq_base=0.0, rope_freq_scale=0.0, yarn_ext_factor=-1.0, 
yarn_attn_factor=1.0, yarn_beta_fast=32.0, yarn_beta_slow=1.0, 
yarn_orig_ctx=0, logits_all=False, embedding=False, 
offload_kqv=True, flash_attn=False, last_n_tokens_size=64, 
lora_base=None, lora_scale=1.0, lora_path=None, numa=False, 
chat_format=None, chat_handler=None, draft_model=None, 
tokenizer=None, type_k=None, type_v=None, spm_infill=False, 
verbose=True, **kwargs
)
```

You may or may not be familiar with these parameters... Well, it doesn't matter. We just want to give the user the ability to set any parameters they want. Let's take a lookd at `load_model()` function from our `backend.py`

```python
def load_model(model_path, params):
    model = Llama(model_path, mlock=params['mlock'], n_ctx=params['n_ctx'], chat_format=params['chat_format'], verbose=params['verbose'])
    return model
```

We've determined wich parameters model should accept... Defining every parameter would take a long time and also adds a lot of useless data to our code. To fix this, we can use `signature` method from `inspect` module. Let's add this to our `backend.py`

```python
from inspect import signature
```

Now let's change our `load_model()` function

```python
def load_model(model_path, params):
    llama_sig = signature(Llama)
    valid_params = {key: value for key, value in params.items() if key in llama_sig.parameters}
    model = Llama(model_path, **valid_params)
    return model
```

Passing the `Llama` class to the `signature` would return all valid parameters that `Llama` wants.

And that's it! We can now add as many parameters as we want... They just should be the exact name of which the `Llama` class gets.

### System Prompt and Characters

Passing system prompt as a CLI argument is... a bit... wild, i guess. Imagine wanting to do a roleplay. In that scenario, the system prompt would be atleast 10 lines. 

We can change it so the script reads the system prompt from a file. Let's first create a folder to save our context files in it.

```shell
(venv)$PyChatbot/src cd ..
(venv)$PyChatbot mkdir contexts
(venv)$PyChatbot cd contexts
```

Let's change `get_sys_prompt()` function in `backend.py`

```python
def get_sys_prompt(name):
    with open('../contexts/' + name, 'r') as c_file:
        return c_file.read().strip('\n')
```

Note: *We want to have multiple characters in case of roleplaying. So every file would saved as its own name as the character name. That's why we pass the `name` argument to our function.*

With stripping `'\n'`s we can shorten the context length.

Now for testing purpose let's change some code in our `run.py` file so it gets the character name from CLI and passes it to the `get_sys_prompt()` function as the name. Change this:

```python
sys_prompt = backend.get_sys_prompt()
```

To this:

```python
sys_prompt = backend.get_sys_prompt(sys.argv[1])
```

Now let's also change this line:

```python
print("[+] AI: " + response)
```

To this line:

```python
print("[+] " + sys.argv[1].capitalize() + ": " + response)
```

So instead of **AI**, the name that user chose for the character displays.

### Generation Parameters

Based on `llama-cpp-python` documents for `create_chat_completion`:

```python
create_chat_completion(messages, functions=None, function_call=None,
tools=None, tool_choice=None, temperature=0.2, top_p=0.95,
top_k=40, min_p=0.05, typical_p=1.0, stream=False, stop=[], 
seed=None, response_format=None, max_tokens=None, presence_penalty=0.0, 
frequency_penalty=0.0, repeat_penalty=1.0, tfs_z=1.0, mirostat_mode=0, 
mirostat_tau=5.0, mirostat_eta=0.1, model=None, logits_processor=None, 
grammar=None, logit_bias=None, logprobs=None, top_logprobs=None)
```

As we can see, there's also a lot of parameters we can pass to the model for better text generation. And we want user to choose what parameters and what values to pass. So we'll do the same thing we did to `load_mode()`. Change `chat_comp()` function in our `backed.py` file: *We'll save generation paramters in PyChatbot/configs/gen_parameters.json*

```python
def chat_comp(messages, model, params):
    gen_sig = signature(model.create_chat_completion)
    valid_params = {key: value for key, value in params.items() if key in gen_sig.parameters}
    model_output = model.create_chat_completion(messages=messages, **valid_params)
    return model_output['choices'][0]['message']['content']
```

So now we can pass any parameter we want to the chat generation.

Also let's change `read_params()` function in our `backend.py` file so we wouldn't have to write multiple functions for reading different file

```python
def read_params(file_name):
    with open('../configs/' + file_name, 'r') as file:
        parameters = json.load(file)
    for key in parameters.keys():
        if parameters[key] == "True":
            parameters[key] = True
        if parameters[key] == "False":
            parameters[key] = False
    return parameters
```

And in our `run.py` Let's change this line:

```python
model_params = backend.read_params()
```

To this:

```python
model_params = backend.read_params('model_parameters.json')
```

And also add this line in our while loop:

```python
gen_params = backend.read_params('gen_parameters.json')
```

And pass `gen_params` to our `chat_comp()` function:

```python
response = backend.chat_comp(messages_list, loaded_model, gen_params)
```

Now let's create a `gen_parameters.json` file in our configs folder:

```json
{
    "temperature": 2,
    "max_tokens": 128,
    "stop": "\n"
}
```

Now let's test these changes really quick. Firstly create a file called assistant in our contexts folder to put our system instruction in it:

```shell
(venv)$PyChatbot/contexts touch assistant
```

```
You're an AI assistant that thinks outside of the box helps user with their tasks.
```

Now let's run the script

```shell
(venv)$PyChatbot/src python run.py assistant
```

*For now, let's remove "stop" parameter from our `gen_parameters.json` file. Because it makes model stop generating after wanting to generate a new line... And it's just good for roleplaying.*

![](C:\Users\darkg\AppData\Roaming\marktext\images\2024-07-26-17-57-01-image.png)

As we can see, it works perfectly.



There's still plenty of changes we need to do in order to make it user-friendly... I think It's enough for now.
